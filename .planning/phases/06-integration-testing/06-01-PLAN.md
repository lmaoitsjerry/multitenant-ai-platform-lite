---
phase: 06-integration-testing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/test_integration_email_pipeline.py
  - tests/test_integration_helpdesk_rag.py
autonomous: true

must_haves:
  truths:
    - "Email -> Tenant -> Parse -> Draft Quote -> Approve -> Send pipeline executes without error"
    - "Helpdesk returns conversational responses with specific details for Zanzibar query"
    - "Helpdesk handles unknown questions gracefully"
    - "All existing unit tests pass (no regressions)"
    - "Response times are within targets (helpdesk < 3s)"
  artifacts:
    - path: "tests/test_integration_email_pipeline.py"
      provides: "E2E tests for email pipeline"
      min_lines: 80
    - path: "tests/test_integration_helpdesk_rag.py"
      provides: "E2E tests for helpdesk RAG"
      min_lines: 60
  key_links:
    - from: "test_integration_email_pipeline.py"
      to: "src/webhooks/email_webhook.py"
      via: "simulate inbound email POST"
      pattern: "receive_inbound_email|process_inbound"
    - from: "test_integration_helpdesk_rag.py"
      to: "src/api/helpdesk_routes.py"
      via: "ask_helpdesk endpoint"
      pattern: "ask_helpdesk|search_with_context"
---

<objective>
Create end-to-end integration tests verifying both critical systems work from start to finish.

Purpose: Confirm all phases (1-5) integrate correctly before production deployment. This is the final quality gate - no new features, just verification that the pipeline works.

Output: Two integration test files that exercise the full email pipeline and helpdesk RAG flow.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

Key prior summaries (for patterns and artifacts):
@.planning/phases/01-diagnostics-logging/01-01-SUMMARY.md
@.planning/phases/04-email-sending-notifications/04-01-SUMMARY.md
@.planning/phases/05-helpdesk-rag-enhancement/05-02-SUMMARY.md

Source files to test:
@src/webhooks/email_webhook.py
@src/api/helpdesk_routes.py
@src/services/faiss_helpdesk_service.py
@src/services/rag_response_service.py
@src/agents/quote_agent.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Email Pipeline Integration Tests</name>
  <files>tests/test_integration_email_pipeline.py</files>
  <action>
Create integration tests for the full email -> quote pipeline. Use pytest with mocks for external services (SendGrid, OpenAI, Supabase), but test the actual flow.

Tests to include:
1. test_full_email_to_draft_quote_pipeline
   - Simulate inbound email POST to /webhooks/email/inbound
   - Mock tenant lookup to return valid tenant
   - Mock LLM parser to return parsed data
   - Mock quote generation dependencies
   - Assert: Quote created with status "draft"

2. test_draft_quote_approve_and_send
   - Mock authenticated user
   - Call POST /api/v1/quotes/{quote_id}/send
   - Mock PDF generation, email sending
   - Assert: Status changed to "sent", notification created

3. test_tenant_not_found_handled_gracefully
   - Send email to unknown address
   - Assert: Returns 200 but logs tenant not found

4. test_email_parse_fallback_to_rules
   - Mock LLM parser to raise exception
   - Assert: Falls back to UniversalEmailParser

Use existing test patterns from tests/test_quote_generation.py and tests/test_email_webhook.py.
  </action>
  <verify>pytest tests/test_integration_email_pipeline.py -v passes all tests</verify>
  <done>4+ integration tests cover the email pipeline end-to-end with proper mocking</done>
</task>

<task type="auto">
  <name>Task 2: Create Helpdesk RAG Integration Tests</name>
  <files>tests/test_integration_helpdesk_rag.py</files>
  <action>
Create integration tests for the helpdesk RAG flow. Mock FAISS index and OpenAI, but test the actual endpoint integration.

Tests to include:
1. test_helpdesk_returns_natural_response
   - Mock FAISS search to return hotel results
   - Mock OpenAI to return synthesized response
   - Call POST /api/v1/helpdesk/ask
   - Assert: Response is string (not list), contains expected hotel name

2. test_helpdesk_unknown_question_graceful
   - Mock FAISS search to return empty results
   - Call POST /api/v1/helpdesk/ask with off-topic question
   - Assert: Response contains "don't have" or similar acknowledgment

3. test_helpdesk_response_includes_timing
   - Make request and check response structure
   - Assert: Response has timing.search_ms, timing.synthesis_ms, timing.total_ms

4. test_helpdesk_llm_failure_fallback
   - Mock OpenAI to raise exception
   - Mock FAISS to return results
   - Assert: Falls back to formatted results (not crash)

Use TestClient from FastAPI for endpoint testing.
  </action>
  <verify>pytest tests/test_integration_helpdesk_rag.py -v passes all tests</verify>
  <done>4+ integration tests cover the helpdesk RAG flow with proper mocking</done>
</task>

<task type="auto">
  <name>Task 3: Run Full Test Suite and Verify No Regressions</name>
  <files>None (verification only)</files>
  <action>
Run the complete test suite to verify no regressions from Phases 1-5.

Steps:
1. Run all existing tests: pytest tests/ -v
2. Count passing vs failing tests
3. If any failures, investigate and report (do not fix in this plan - just document)
4. Verify test count includes:
   - test_email_webhook.py (10 tests)
   - test_email_parser.py (16 tests)
   - test_quote_generation.py (13 tests)
   - test_integration_email_pipeline.py (4+ new tests)
   - test_integration_helpdesk_rag.py (4+ new tests)

Do NOT mock environment in a way that breaks other tests. Use pytest fixtures with proper scoping.
  </action>
  <verify>pytest tests/ -v shows all tests passing (47+ tests, 0 failures)</verify>
  <done>Full test suite passes, no regressions from prior phases</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Unit tests intact:** pytest tests/test_*.py (existing tests pass)
2. **Integration tests pass:** pytest tests/test_integration_*.py -v
3. **Email pipeline tested:** Full flow from inbound email to sent quote
4. **Helpdesk RAG tested:** Natural responses with timing data
5. **No regressions:** Test count increased (not decreased)
</verification>

<success_criteria>
- [ ] tests/test_integration_email_pipeline.py exists with 4+ tests
- [ ] tests/test_integration_helpdesk_rag.py exists with 4+ tests
- [ ] All integration tests pass: pytest tests/test_integration_*.py -v
- [ ] All tests pass: pytest tests/ -v (47+ tests, 0 failures)
- [ ] No mock leakage between tests (each test isolated)
</success_criteria>

<output>
After completion, create `.planning/phases/06-integration-testing/06-01-SUMMARY.md`
</output>
