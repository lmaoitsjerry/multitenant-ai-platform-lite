---
phase: 06-integration-testing
plan: 02
type: execute
wave: 2
depends_on: ["06-01"]
files_modified: []
autonomous: false
user_setup:
  - service: SendGrid
    why: "Verify Inbound Parse configuration"
    dashboard_config:
      - task: "Confirm Inbound Parse webhook URL points to production"
        location: "SendGrid Dashboard -> Settings -> Inbound Parse"
      - task: "Verify MX records for inbound.zorah.ai"
        location: "DNS provider / SendGrid dashboard"

must_haves:
  truths:
    - "All unit and integration tests pass in clean environment"
    - "Helpdesk responds naturally to real question (human verified)"
    - "Email pipeline configuration is correct (SendGrid Inbound Parse)"
    - "Production deployment checklist completed"
  artifacts: []
  key_links: []
---

<objective>
Final verification of both systems with human testing and deployment readiness confirmation.

Purpose: This is the ship-gate. All automated tests passed in 06-01. Now we need human verification of natural language quality and confirmation that production infrastructure (SendGrid Inbound Parse, MX records) is properly configured.

Output: Human-verified confirmation that both systems work as expected.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

Prior plan must complete:
@.planning/phases/06-integration-testing/06-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Generate Deployment Readiness Report</name>
  <files>None (stdout report)</files>
  <action>
Run automated checks and generate a deployment readiness report. Output to console.

Checks to run:
1. Python syntax check: python -m py_compile on all modified files
2. Test suite: pytest tests/ -v --tb=short
3. Import check: python -c "from main import app" (FastAPI loads)
4. Environment check: List required env vars (OPENAI_API_KEY, SENDGRID_API_KEY, etc.)

Generate report with:
- Total tests: X passed, Y failed
- Syntax: All files valid
- Import: App loads successfully
- Env vars needed for production

Do NOT deploy - just report readiness.
  </action>
  <verify>Report generated showing test results and import success</verify>
  <done>Deployment readiness report printed to console</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete Email Pipeline:
- Email -> Tenant Lookup -> LLM Parse -> Draft Quote -> Approve -> Send
- Dashboard notification on quote sent

Complete Helpdesk RAG:
- FAISS search with context (5-8 docs)
- GPT-4o-mini synthesis to natural responses
- Timing instrumentation (< 3s target)
  </what-built>
  <how-to-verify>
**Test 1: Helpdesk Natural Language Quality**
1. Start the server: uvicorn main:app --reload
2. Use curl or browser to test:
   ```
   curl -X POST http://localhost:8000/api/v1/helpdesk/ask \
     -H "Content-Type: application/json" \
     -H "X-Client-ID: {your_test_tenant}" \
     -d '{"query": "What hotels do you have in Zanzibar with beach access?"}'
   ```
3. Verify response is:
   - Conversational (not a bullet list)
   - Mentions specific hotel names
   - Has timing data in response

**Test 2: Helpdesk Unknown Question Handling**
1. Ask an off-topic question:
   ```
   curl -X POST http://localhost:8000/api/v1/helpdesk/ask \
     -H "Content-Type: application/json" \
     -H "X-Client-ID: {your_test_tenant}" \
     -d '{"query": "What is the capital of France?"}'
   ```
2. Verify response gracefully acknowledges lack of knowledge

**Test 3: Email Pipeline (if SendGrid Inbound Parse is configured)**
1. Send test email to a tenant's SendGrid address (e.g., final-itc-3@zorah.ai)
2. Check Cloud Run logs for diagnostic steps 1-11
3. Check tenant dashboard for draft quote
4. Approve quote and verify email sent

**Test 4: SendGrid Configuration Check**
1. Login to SendGrid Dashboard
2. Go to Settings -> Inbound Parse
3. Verify webhook URL matches production API
4. Verify MX records for inbound domain
  </how-to-verify>
  <resume-signal>
Type one of:
- "approved" - All systems verified working
- "helpdesk-issue: {description}" - Helpdesk needs adjustment
- "email-issue: {description}" - Email pipeline needs adjustment
- "sendgrid-config: {description}" - SendGrid configuration issue (out of scope - infrastructure)
  </resume-signal>
</task>

<task type="auto">
  <name>Task 3: Update PROJECT.md and STATE.md with Completion</name>
  <files>None (conditional on approval)</files>
  <action>
ONLY execute this task if human verification was "approved".

Update .planning/PROJECT.md:
- Move all EMAIL-* requirements from "Active" to "Validated"
- Move all RAG-* requirements from "Active" to "Validated"
- Update "Last updated" timestamp

Update .planning/STATE.md:
- Set phase to "Complete"
- Set progress to 100%
- Add summary of milestone v2.0 completion

If human reported an issue, do NOT update these files. The issue will need a gap closure plan.
  </action>
  <verify>If approved: PROJECT.md shows EMAIL-* and RAG-* as Validated. If issue reported: Files unchanged.</verify>
  <done>Project artifacts reflect current state (complete or issue noted)</done>
</task>

</tasks>

<verification>
Phase 6 complete when:

1. **Automated tests pass:** 47+ tests, 0 failures
2. **Human verified helpdesk:** Natural responses to Zanzibar query
3. **Human verified email:** Pipeline works or SendGrid config issue documented
4. **Project updated:** Requirements marked validated (if approved)
</verification>

<success_criteria>
- [ ] Deployment readiness report generated
- [ ] Human tested helpdesk with Zanzibar query
- [ ] Human tested helpdesk with unknown question
- [ ] Human verified SendGrid Inbound Parse configuration
- [ ] Human provided "approved" or documented specific issues
- [ ] PROJECT.md updated if approved (EMAIL-*, RAG-* validated)
</success_criteria>

<output>
After completion, create `.planning/phases/06-integration-testing/06-02-SUMMARY.md`
</output>
