---
phase: 05-helpdesk-rag-enhancement
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/services/rag_response_service.py
  - src/api/helpdesk_routes.py
autonomous: true

must_haves:
  truths:
    - "Response is conversational, not list-like"
    - "Response includes specific details from context (names, prices, features)"
    - "Unknown questions handled gracefully with honest acknowledgment"
    - "Response time under 3 seconds"
  artifacts:
    - path: "src/services/rag_response_service.py"
      provides: "LLM-powered RAG synthesis"
      exports: ["RAGResponseService", "generate_rag_response"]
      min_lines: 80
    - path: "src/api/helpdesk_routes.py"
      provides: "Integration with RAG response service"
      contains: "generate_rag_response"
  key_links:
    - from: "src/api/helpdesk_routes.py"
      to: "src/services/rag_response_service.py"
      via: "generate_rag_response import and call"
      pattern: "from src.services.rag_response_service import"
    - from: "src/services/rag_response_service.py"
      to: "OpenAI API"
      via: "openai.OpenAI client"
      pattern: "openai\\.OpenAI"
---

<objective>
Implement LLM-powered RAG response synthesis for natural, helpful answers

Purpose: Transform robotic list dumps into conversational responses that include specific details from the knowledge base.

Output: RAG service that synthesizes natural responses from FAISS search results using GPT-4o-mini.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@src/services/faiss_helpdesk_service.py
@src/api/helpdesk_routes.py
@src/agents/llm_email_parser.py (reference for OpenAI pattern)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create RAG response synthesis service</name>
  <files>src/services/rag_response_service.py</files>
  <action>
Create new file `src/services/rag_response_service.py`:

```python
"""
RAG Response Service - Natural Language Synthesis

Transforms FAISS search results into conversational, helpful responses
using GPT-4o-mini. Handles unknown questions gracefully.
"""

import os
import logging
from typing import List, Dict, Any, Optional

logger = logging.getLogger(__name__)


class RAGResponseService:
    """Synthesize natural responses from knowledge base search results"""

    def __init__(self):
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self._client = None

    @property
    def client(self):
        """Lazy-load OpenAI client"""
        if self._client is None and self.openai_api_key:
            import openai
            self._client = openai.OpenAI(api_key=self.openai_api_key)
        return self._client

    def generate_response(
        self,
        question: str,
        search_results: List[Dict[str, Any]],
        max_context_chars: int = 6000
    ) -> Dict[str, Any]:
        """
        Generate a natural response from search results.

        Args:
            question: User's original question
            search_results: List of dicts with 'content', 'score', 'source'
            max_context_chars: Maximum characters of context to include

        Returns:
            Dict with 'answer', 'sources', 'method' ('rag' or 'fallback')
        """
        # No API key - return structured fallback
        if not self.client:
            logger.warning("No OpenAI API key, cannot synthesize RAG response")
            return self._fallback_response(question, search_results)

        # No search results - handle gracefully
        if not search_results:
            return self._no_results_response(question)

        # Build context from search results
        context = self._build_context(search_results, max_context_chars)

        # Generate response
        try:
            answer = self._call_llm(question, context)
            return {
                'answer': answer,
                'sources': [
                    {'filename': r.get('source', 'Knowledge Base'), 'score': r.get('score', 0)}
                    for r in search_results[:5]  # Top 5 sources
                ],
                'method': 'rag'
            }
        except Exception as e:
            logger.error(f"LLM synthesis failed: {e}")
            return self._fallback_response(question, search_results)

    def _build_context(self, results: List[Dict], max_chars: int) -> str:
        """Build context string from search results"""
        context_parts = []
        total_chars = 0

        for i, r in enumerate(results):
            content = r.get('content', '')
            source = r.get('source', 'Document')

            # Truncate individual documents if too long
            if len(content) > 1500:
                content = content[:1500] + "..."

            part = f"[Source: {source}]\n{content}"

            if total_chars + len(part) > max_chars:
                break

            context_parts.append(part)
            total_chars += len(part)

        return "\n\n---\n\n".join(context_parts)

    def _call_llm(self, question: str, context: str) -> str:
        """Call GPT-4o-mini to synthesize response"""
        system_prompt = """You are a helpful travel assistant for a property management platform. Your job is to answer questions using ONLY the provided context from the knowledge base.

Guidelines:
1. Be conversational and friendly, not robotic or list-like
2. Include SPECIFIC details from the context (hotel names, prices, features, locations)
3. If the context doesn't fully answer the question, acknowledge what you know and what's missing
4. If the context has NO relevant information, honestly say you don't have that information
5. Keep responses concise but informative (2-4 paragraphs max)
6. Use natural language, not bullet points unless listing options
7. If recommending properties, explain WHY they fit the query

DO NOT:
- Make up information not in the context
- Give generic travel advice not from the documents
- Use corporate jargon or robotic language
- Start with "Based on the context..." or similar meta-language"""

        user_prompt = f"""Question: {question}

Context from knowledge base:
{context}

Provide a helpful, natural response using the information above. If the context doesn't contain relevant information, honestly acknowledge that."""

        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7,  # Natural variation
            max_tokens=500,
            timeout=8.0  # Stay under 3s total target
        )

        return response.choices[0].message.content

    def _fallback_response(self, question: str, results: List[Dict]) -> Dict[str, Any]:
        """Fallback when LLM unavailable - return formatted results"""
        if not results:
            return self._no_results_response(question)

        # Simple concatenation with intro (existing behavior)
        combined = "\n\n".join([r.get('content', '')[:500] for r in results[:3]])
        answer = f"Here's what I found in the knowledge base:\n\n{combined}\n\nFor more specific information, please refine your question."

        return {
            'answer': answer,
            'sources': [{'filename': r.get('source', 'Knowledge Base'), 'score': r.get('score', 0)} for r in results[:3]],
            'method': 'fallback'
        }

    def _no_results_response(self, question: str) -> Dict[str, Any]:
        """Graceful handling when no relevant documents found"""
        answer = (
            "I don't have specific information about that in my knowledge base. "
            "This could be because:\n\n"
            "- The topic isn't covered in our documentation yet\n"
            "- Try rephrasing your question with different keywords\n"
            "- For specific property or rate questions, the information might be in our pricing system instead\n\n"
            "Is there something else I can help you with, or would you like me to search for related topics?"
        )
        return {
            'answer': answer,
            'sources': [],
            'method': 'no_results'
        }


# Singleton instance
_rag_service = None


def get_rag_service() -> RAGResponseService:
    """Get singleton RAG service instance"""
    global _rag_service
    if _rag_service is None:
        _rag_service = RAGResponseService()
    return _rag_service


def generate_rag_response(question: str, search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Convenience function for generating RAG responses"""
    service = get_rag_service()
    return service.generate_response(question, search_results)
```

Key design decisions:
- Temperature 0.7 for natural variation (per PROJECT.md)
- 8 second timeout to stay under 3s total with network variance
- Graceful fallback if OpenAI unavailable
- Honest "I don't know" for unknown questions (RAG-04)
- Specific details emphasized in prompt (RAG-03)
  </action>
  <verify>
Add test at bottom of file:
```python
if __name__ == "__main__":
    # Test with mock results
    mock_results = [
        {'content': 'The Maldives offers luxury overwater villas at Soneva Fushi starting at $1,200/night. Features include private pools, butler service, and direct lagoon access.', 'score': 0.85, 'source': 'maldives_hotels.md'},
        {'content': 'For families, Conrad Maldives has a kids club and two-bedroom villas. Rates from $800/night with meal plans available.', 'score': 0.78, 'source': 'maldives_family.md'}
    ]

    service = RAGResponseService()
    result = service.generate_response("What hotels in Maldives have pools?", mock_results)
    print(f"Method: {result['method']}")
    print(f"Answer:\n{result['answer']}")
```
Run: `python src/services/rag_response_service.py`
  </verify>
  <done>RAGResponseService synthesizes natural responses from search results</done>
</task>

<task type="auto">
  <name>Task 2: Integrate RAG service into helpdesk routes</name>
  <files>src/api/helpdesk_routes.py</files>
  <action>
Update helpdesk_routes.py to use the new RAG service:

1. Add import at top (after existing imports):
```python
from src.services.rag_response_service import generate_rag_response
```

2. Replace `format_knowledge_response()` function (around line 260) with:
```python
def format_knowledge_response(results: List[Dict], question: str) -> Dict[str, Any]:
    """
    Format knowledge base results using RAG synthesis.
    Returns dict with 'answer', 'sources', 'method'.
    """
    return generate_rag_response(question, results)
```

3. Update the ask_helpdesk endpoint (around line 343) to use the new return format:
```python
if kb_results:
    # Use RAG synthesis for natural response
    rag_response = format_knowledge_response(kb_results, question)

    return {
        "success": True,
        "answer": rag_response['answer'],
        "sources": [
            {
                "filename": s.get("filename", "Knowledge Base"),
                "score": s.get("score", 0),
                "type": "knowledge_base"
            }
            for s in rag_response.get('sources', [])
        ],
        "method": rag_response.get('method', 'rag')
    }
```

4. Update test-search endpoint to also show synthesized response:
Add after the results section in test_faiss_search():
```python
# Also show RAG synthesis preview
from src.services.rag_response_service import generate_rag_response
rag_result = generate_rag_response(q, results[:3])
```
And include `"synthesized_preview": rag_result['answer'][:300] + "..."` in the response.

This integrates RAG synthesis while maintaining backwards compatibility with the API response format.
  </action>
  <verify>
1. Backend starts without errors: `python main.py`
2. Test natural response:
   curl -X POST http://localhost:8000/api/v1/helpdesk/ask \
     -H "Content-Type: application/json" \
     -d '{"question": "What hotels do you have in Zanzibar with beach access?"}'

Expected: Natural, conversational response mentioning specific hotel names and features, not a bullet list.

3. Test unknown question handling:
   curl -X POST http://localhost:8000/api/v1/helpdesk/ask \
     -H "Content-Type: application/json" \
     -d '{"question": "What is the airspeed velocity of an unladen swallow?"}'

Expected: Graceful "I don't have that information" response.
  </verify>
  <done>Helpdesk returns natural, conversational responses with specific details</done>
</task>

<task type="auto">
  <name>Task 3: Add response time logging and validation</name>
  <files>src/api/helpdesk_routes.py</files>
  <action>
Add timing instrumentation to ensure < 3 second response time:

1. Add import at top:
```python
import time
```

2. Update ask_helpdesk to log timing:
```python
@helpdesk_router.post("/ask")
async def ask_helpdesk(
    request: AskQuestion,
    user: Optional[dict] = Depends(get_current_user_optional),
    config: ClientConfig = Depends(get_client_config)
):
    """..."""
    start_time = time.time()

    try:
        question = request.question

        # Step 1: Try knowledge base search
        search_start = time.time()
        kb_results = search_knowledge_base(config, question)
        search_time = time.time() - search_start

        if kb_results:
            # Step 2: RAG synthesis
            synth_start = time.time()
            rag_response = format_knowledge_response(kb_results, question)
            synth_time = time.time() - synth_start

            total_time = time.time() - start_time
            logger.info(f"Helpdesk RAG: search={search_time:.2f}s, synth={synth_time:.2f}s, total={total_time:.2f}s")

            if total_time > 3.0:
                logger.warning(f"Helpdesk response exceeded 3s target: {total_time:.2f}s")

            return {
                "success": True,
                "answer": rag_response['answer'],
                "sources": [...],
                "method": rag_response.get('method', 'rag'),
                "timing": {
                    "search_ms": int(search_time * 1000),
                    "synthesis_ms": int(synth_time * 1000),
                    "total_ms": int(total_time * 1000)
                }
            }

        # ... rest of function with similar timing for fallback path
```

This provides:
- Visibility into performance bottlenecks
- Warning logs when response exceeds 3s target
- Timing data in response for frontend debugging
  </action>
  <verify>
curl -X POST http://localhost:8000/api/v1/helpdesk/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "Maldives hotels"}' | jq '.timing'

Expected: timing object with search_ms, synthesis_ms, total_ms all reasonable
Check logs for: "Helpdesk RAG: search=X.XXs, synth=X.XXs, total=X.XXs"
  </verify>
  <done>Response time logged and validated against 3-second target</done>
</task>

</tasks>

<verification>
1. Service file exists and runs: `python src/services/rag_response_service.py`
2. Backend starts: `python main.py` - no import errors
3. Natural response test:
   - Ask about Zanzibar hotels -> get conversational response with hotel names
   - Ask about pricing -> get response with specific rates mentioned
4. Unknown question test:
   - Ask random question -> get honest "I don't know" response
5. Performance test:
   - Response timing.total_ms < 3000 for typical queries
   - No timeout warnings in logs
</verification>

<success_criteria>
- RAGResponseService exists with generate_response() method
- Helpdesk /ask endpoint returns natural, conversational responses
- Responses include specific details (names, prices) from documents
- Unknown questions get graceful "I don't know" acknowledgment
- Response time < 3 seconds (measured and logged)
</success_criteria>

<output>
After completion, create `.planning/phases/05-helpdesk-rag-enhancement/05-02-SUMMARY.md`
</output>
