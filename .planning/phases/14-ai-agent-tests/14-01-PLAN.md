---
phase: 14-ai-agent-tests
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/fixtures/openai_fixtures.py
  - tests/test_helpdesk_agent.py
autonomous: true

must_haves:
  truths:
    - "OpenAI API responses can be mocked deterministically"
    - "Helpdesk agent tests cover conversation flow"
    - "Tool calling (function calling) can be tested without real API"
  artifacts:
    - path: "tests/fixtures/openai_fixtures.py"
      provides: "Reusable OpenAI mock infrastructure"
      exports: ["MockOpenAIResponse", "MockOpenAIClient", "create_mock_openai_client"]
    - path: "tests/test_helpdesk_agent.py"
      provides: "Helpdesk agent test coverage"
      min_lines: 200
  key_links:
    - from: "tests/test_helpdesk_agent.py"
      to: "src/agents/helpdesk_agent.py"
      via: "import and mock OpenAI client"
      pattern: "patch.*openai"
---

<objective>
Create reusable OpenAI mock infrastructure and comprehensive tests for the helpdesk agent.

Purpose: Enable deterministic testing of LLM-powered agents without making real API calls. The helpdesk agent orchestrates knowledge base search, quote generation, platform help, and human routing via OpenAI function calling - all paths need test coverage.

Output: OpenAI mock fixtures module + helpdesk agent tests achieving 50%+ coverage (from current 0%)
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@tests/fixtures/bigquery_fixtures.py (pattern reference)
@tests/fixtures/sendgrid_fixtures.py (pattern reference)
@src/agents/helpdesk_agent.py (module to test)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create OpenAI mock infrastructure</name>
  <files>tests/fixtures/openai_fixtures.py</files>
  <action>
Create reusable OpenAI mock classes following the BigQuery/SendGrid fixture patterns:

1. **MockOpenAIMessage** class:
   - Simulates `response.choices[0].message` structure
   - Attributes: `content`, `tool_calls`, `role`
   - `tool_calls` is list of MockToolCall objects (or None)

2. **MockToolCall** class:
   - Attributes: `id`, `type` (always "function"), `function`
   - `function` has `name` and `arguments` (JSON string)

3. **MockOpenAIChoice** class:
   - Attributes: `index`, `message` (MockOpenAIMessage), `finish_reason`

4. **MockOpenAIResponse** class:
   - Simulates `client.chat.completions.create()` response
   - Attributes: `id`, `model`, `choices` (list of MockOpenAIChoice), `usage`

5. **MockOpenAIClient** class:
   - Supports `chat.completions.create()` method signature
   - Pattern-based response matching (match on message content keywords)
   - Configurable responses for different scenarios:
    - Direct response (no tool calls)
    - Tool call response (search_knowledge_base, start_quote, platform_help, route_to_human)
    - Error scenarios

6. Factory function: `create_mock_openai_client(responses: Dict[str, MockOpenAIResponse])`

7. Helper generators:
   - `create_direct_response(content: str)` - Returns response without tool calls
   - `create_tool_call_response(tool_name: str, arguments: dict)` - Returns response with function call
   - `create_search_response(query: str)` - Preset for search_knowledge_base tool
   - `create_quote_response(destination: str)` - Preset for start_quote tool

Include docstrings and usage examples matching the BigQuery fixtures style.
  </action>
  <verify>
```bash
python -c "from tests.fixtures.openai_fixtures import MockOpenAIClient, create_mock_openai_client, create_tool_call_response; print('OpenAI fixtures import OK')"
```
  </verify>
  <done>OpenAI mock infrastructure exists and is importable with all documented classes and functions</done>
</task>

<task type="auto">
  <name>Task 2: Create helpdesk agent tests</name>
  <files>tests/test_helpdesk_agent.py</files>
  <action>
Create comprehensive tests for `src/agents/helpdesk_agent.py` using the OpenAI fixtures.

**Test classes to implement:**

1. **TestHelpdeskAgentInit**:
   - `test_init_with_config` - Agent initializes with ClientConfig
   - `test_init_without_config` - Agent initializes with None config
   - `test_client_lazy_loading` - OpenAI client not created until accessed
   - `test_client_not_created_without_api_key` - Returns None when OPENAI_API_KEY missing

2. **TestHelpdeskAgentChat**:
   - `test_chat_direct_response` - Message gets direct response (no tool call)
   - `test_chat_triggers_search_tool` - Travel question triggers search_knowledge_base
   - `test_chat_triggers_quote_tool` - Booking request triggers start_quote
   - `test_chat_triggers_platform_help` - Platform question triggers platform_help
   - `test_chat_triggers_human_routing` - Complex issue triggers route_to_human
   - `test_chat_fallback_when_no_client` - Returns fallback when OpenAI unavailable
   - `test_chat_fallback_on_error` - Returns fallback on API error
   - `test_conversation_history_updated` - History tracks messages

3. **TestToolExecution**:
   - `test_execute_search_with_results` - Search returns knowledge base content
   - `test_execute_search_no_results` - Search handles empty results
   - `test_execute_search_error` - Search handles service errors gracefully
   - `test_execute_start_quote_with_destination` - Quote flow with destination
   - `test_execute_start_quote_missing_dates` - Quote flow missing dates asks for them
   - `test_execute_start_quote_missing_destination` - Quote flow missing destination
   - `test_execute_platform_help_quotes` - Platform help returns quote guidance
   - `test_execute_platform_help_invoices` - Platform help returns invoice guidance
   - `test_execute_platform_help_crm` - Platform help returns CRM guidance
   - `test_execute_route_to_human` - Human routing returns reference number

4. **TestConversationManagement**:
   - `test_reset_conversation` - Clears history and tool calls
   - `test_max_history_trimming` - History trimmed at max_history limit
   - `test_get_stats` - Returns conversation statistics

5. **TestSingletonHelpers**:
   - `test_get_helpdesk_agent_singleton` - Returns same instance
   - `test_reset_helpdesk_agent` - Resets singleton to None

**Mocking strategy:**
- Patch `openai.OpenAI` at `src.agents.helpdesk_agent.openai.OpenAI`
- Patch RAG services at their import locations
- Use `@pytest.fixture` for mock config matching conftest.py patterns
- Set `OPENAI_API_KEY` env var via `patch.dict(os.environ, ...)`

Target: 25+ tests covering all major code paths, achieving 50%+ coverage on helpdesk_agent.py
  </action>
  <verify>
```bash
cd "C:\Users\jerry\Documents\multitenant-ai-platform-lite" && python -m pytest tests/test_helpdesk_agent.py -v --tb=short 2>&1 | head -80
```
  </verify>
  <done>All helpdesk agent tests pass; coverage of helpdesk_agent.py reaches 50%+</done>
</task>

<task type="auto">
  <name>Task 3: Verify coverage improvement</name>
  <files>tests/test_helpdesk_agent.py</files>
  <action>
Run coverage report specifically for helpdesk_agent.py to verify the 50%+ target:

1. Run pytest with coverage on just the helpdesk agent tests
2. Check the coverage percentage for src/agents/helpdesk_agent.py
3. If below 50%, identify uncovered lines and add targeted tests:
   - Look for error handling paths not covered
   - Look for edge cases in tool execution
   - Look for conditional branches not hit

Add any additional tests needed to cross the 50% threshold.
  </action>
  <verify>
```bash
cd "C:\Users\jerry\Documents\multitenant-ai-platform-lite" && python -m pytest tests/test_helpdesk_agent.py --cov=src/agents/helpdesk_agent --cov-report=term-missing 2>&1 | tail -30
```
  </verify>
  <done>Coverage report shows src/agents/helpdesk_agent.py at 50%+ coverage</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. `python -c "from tests.fixtures.openai_fixtures import *"` imports without error
2. `pytest tests/test_helpdesk_agent.py -v` shows 25+ tests passing
3. Coverage report shows helpdesk_agent.py at 50%+
</verification>

<success_criteria>
- OpenAI mock fixtures module created with documented classes
- Helpdesk agent tests achieve 50%+ coverage (from 0%)
- All tests pass without making real API calls
- Mock infrastructure is reusable for other agent tests
</success_criteria>

<output>
After completion, create `.planning/phases/14-ai-agent-tests/14-01-SUMMARY.md`
</output>
